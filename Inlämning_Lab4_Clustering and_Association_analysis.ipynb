{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 - Klustring och Associationsanalys \n",
    "\n",
    "\n",
    "I denna labb ska ni använda två olika typer av oövervakad inlärning för att analysera transaktionsdata.\n",
    "\n",
    "I del 1 kommer ni analysera data från genom så kallad RFM analysis, vi kommer genomföra analysen med hjälp a en K-means modell i Python för att klustra data beroende på det mönster som uppvisas i data, baserat på just RFM.\n",
    "\n",
    "I del 2 ska ni få testa på associationsanalys genom att  analysera transaktionsdata från matvaruhandeln, närmare bestämt för en butik kallad *MatFörAlla* för att på detta sätt komma med råd för hur de bör organisera sin butik för att öka försäljningen. \n",
    "\n",
    "Till skillnad från flera av de tidigare laborationerna kommer fokus här inte vara så mycket på programmeringen, utan mycket kod kommer redan finnas i laborationen. Fokus är snarare på att förstå de baokmliggande mekanismerna i denna typ av dataanalys som skiljer sig från de vi tidigare tittat på. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del 1 Klustring\n",
    "\n",
    "Inspirerad av: Khalid i Towards Data Science\n",
    "\n",
    "I denna del av laborationen ska ni använda ett dataset med data från en onlineaffär för att lista ut vilka olika kunder ni har- Vilka är exempelvis lojala kunder,som ni bör ägna extra energi åt, och vilka är sådana som bara handlar någon enstaka gång och som det inte är värt att lägga tid och pengar på? Vi ska alltså segmentera våra kunder baserat på några olika kriterier. Själva segmenteringen kommer vi utföra med K-means clustering. \n",
    "\n",
    "Det finns många olika sätt att segmentera kunder på, några man kan tänka sig är:\n",
    "\n",
    "* Demografiskt - ålder, kön etc. Demographic — age, gender, socioeconomic status\n",
    "* Geografiskt - var de borGeographic — where in the world are they?\n",
    "* **Beteendemässigt - vad gör dina kunder på din sida?** \n",
    "\n",
    "I denna laboration  ska vi segmentera kunder baserat på beteende. Ett vanligt sätt att fånga beteende är genom så kallad RFM segmentering, vilket innebär att kunder segmenteras baserat på tre kriterier nämligen *Recency, Frequency and Monetary*, alltså **RFM**. \n",
    "\n",
    "* Recency - antalet dagar sedan en kund köpte något.\n",
    "* Frequency - Hur ofta en kund handlar\n",
    "* Monetary - För hur mycket har kunden handlat för?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F1** Ni kan själva hämta det dataset ni ska använda [här](https://archive.ics.uci.edu/ml/datasets/online+retail) där finns också värdefull information om kvaliteten på detta dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Online Retail.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['CustomerID'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi kommer inte använda hela detta dataset då det är extremt stort, vilket gör att det blir för tungt att arbeta med. Istället väljer vi ut 10000 rader som får representera våra kunders beteende."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fix = df.sample(10000, random_state = 42)\n",
    "df_fix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Som vanligt behöver vi först bekanta oss med den data vi har tillgänglig och även kontrollera om vi behöver åtgärda något innan vi påbörjar analysen. \n",
    "\n",
    "Följande punkter är krav som behöver säkerställas för att K-means ska fungera på ett dataset. \n",
    "\n",
    "**Inga nullvärden** K-means kan inte hantera nullvärden överhuvudtaget så de måste antingen ersättas eller så måste hela raden tas bort. \n",
    "\n",
    "**Endast numeriska värden.** Eftersom K-means använder distans som beräkningssätt för att hitta kluster fungerar inte kategoriska variabler. Dessa behöver därför ersättas, antingen med numeriska värden, om de tillhör ordnialskala (och därmed har ordning) eller med dummyvariabler om de tillhör nominalskala och alltså inte har någon ordning. Mycket viktigt att inte distanser förändras!\n",
    "\n",
    "**Inga outliers eller brus.** K-means är väldigt känslig för detta.\n",
    "\n",
    "**Det finns ingen/liten korrelation mellan variablerna** Korrelerade variabler är att betrakta som brus och är inte meningsfulla för algoritmer som ska dela in data i olika segment eftersom de representerar samma karaktäristik hos ett segment. Variabler med hög korrelation mellan varandra bör alltså inte båda inkluderas.\n",
    "\n",
    "**Inte för många dimensioner.** När antalet dimensioner (variabler) ökar, kommer distansen mellan alla variabler att konvergera till ett konstant värde mellan vilka variabler som helst, därmed kan man inte längre urskilja några kluster. Ju fler dimensioner, desto svårare att hitta kluster alltså. \n",
    "\n",
    "Det finns också några till, men de kommer vi till senare.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F2** \n",
    "\n",
    ">\n",
    ">a. Är det någon av dessa krav som **inte** är uppfyllda? Motivera svaret.\n",
    ">\n",
    ">b. Hantera eventuella nullvärden på lämpligt sätt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RFM\n",
    "\n",
    "För att segmentera våra kunder ka vi alltså använda RFM,vilket betyder att vi behöver skapa dessa features (kolumner) för att kunna klustra dem i enlighet med dessa. Vi behöver såklart utgå från det data vi har för att skapa dessa tre kolumner. Detta görs i koden nedan. \n",
    "\n",
    "**F3** Kommentera koden så att det är förståeligt vad som händer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime,timedelta\n",
    "df_fix[\"InvoiceDate\"] = df_fix[\"InvoiceDate\"].dt.date\n",
    "df_fix[\"TotalSum\"] = df_fix[\"Quantity\"] * df_fix[\"UnitPrice\"]\n",
    "\n",
    "snapshot_date = max(df_fix.InvoiceDate) + timedelta(days=1)\n",
    "\n",
    "customers = df_fix.groupby(['CustomerID']).agg({\n",
    "   'InvoiceDate': lambda x: (snapshot_date - x.max()).days,\n",
    "    'InvoiceNo': 'count',\n",
    "    'TotalSum': 'sum'})\n",
    "\n",
    "customers.rename(columns = {'InvoiceDate': 'Recency',\n",
    "                           'InvoiceNo': 'Frequency',\n",
    "                           'TotalSum': 'MonetaryValue'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Förutom de krav som diskuterats ovan kräver också K-means att data skalas och transformeras så att det är normalfördelat och standardiserat. \n",
    "\n",
    "**Symetrisk distribution av variabler i data (ingen skevhet).** Transformering av data till en normal distribution medför att outliers och brus från mindre påverkan. \n",
    "\n",
    "**Variabler i samma skala** — Alltså ha samma medelvärde och varians, vanligtvis vill man ha värden mellan -1 till 1 (standardiserad data) eller mellan 0 till 1 (normliserad data). För att K-means ska behandla alla attribut som likvärdiga måste de ha samma skala. \n",
    "\n",
    "I nedanstående kodblock följer en ananlys för att ta reda på hur våra RFM-variabler ser ut.\n",
    "\n",
    "Vi startar med att titta på hur de tre variablerna ser ut."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualisera distributionen av data i våra tre variabler \n",
    "fig, ax = plt.subplots(1, 3, figsize=(15,3))\n",
    "sns.distplot(customers['Recency'], ax=ax[0])\n",
    "sns.distplot(customers['Frequency'], ax=ax[1])\n",
    "sns.distplot(customers['MonetaryValue'], ax=ax[2])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F4** Hur ser vår data ut? Är de tre variablernas distribution symetrisk?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "För att ytterligare undersöka skevheten kan vi också använda pandas inbyggda funktion för det `pd.skew()`. \n",
    "\n",
    "Det går inte att säga exakt vilket värde som reflekterar symmentri även fast man tycka att ett värde på 0 torde göra det, men  det kan också implicera att den ena sidan är \"tjock\" och den andra \"tunn\". En tumregel är dock att ett positivt värde indikerar en positiv skevhet, (alltså att värdena tenderar att vara fler över 0 jämfört med under 0) medan ett negativt värde indikerar en negativ skevhet.\n",
    "\n",
    "Anledningar till skevhet kan vara outliers, men det behöver inte vara så. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers['Recency'].skew().round(2))\n",
    "print(customers['Frequency'].skew().round(2))\n",
    "print(customers['MonetaryValue'].skew().round(2)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F5** Följande kod hanterar de problem som finns gällande skevheten i de tre variablerna. Hur har skevheten hos de tre variablerna förändrats efter transformeringen?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "customers_fix = pd.DataFrame()\n",
    "customers_fix[\"Recency\"] = stats.boxcox(customers['Recency'])[0]\n",
    "customers_fix[\"Frequency\"] = stats.boxcox(customers['Frequency'])[0]\n",
    "customers_fix[\"MonetaryValue\"] = pd.Series(np.cbrt(customers['MonetaryValue'])).values\n",
    "customers_fix.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(customers_fix['Recency'].skew().round(2))\n",
    "print(customers_fix['Frequency'].skew().round(2))\n",
    "print(customers_fix['MonetaryValue'].skew().round(2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi behöver också se till att kravet på variablerna att befinna sig i samma skala uppfylls. Det görs i nedanstående kod. \n",
    "\n",
    "**F6** \n",
    "\n",
    ">\n",
    ">a. Vilken typ av transformering har gjorts? Varför?\n",
    ">\n",
    ">b. Visualisera resultatet\n",
    ">\n",
    ">c. Varför är det så viktigt i K-means att denna pre-processing görs?\n",
    ">\n",
    ">d. Vad för struktur är customers_normalized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(customers_fix)\n",
    "customers_normalized = scaler.transform(customers_fix)\n",
    "print(customers_normalized.mean(axis = 0).round(2))\n",
    "print(customers_normalized.std(axis = 0).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu är det dags att testa att använda K-means för att segmentera våra kunder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "sse = {} #sum of squared error\n",
    "for k in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(customers_normalized)\n",
    "    sse[k] = kmeans.inertia_ # SSE to closest cluster centroid\n",
    "\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('SSE')\n",
    "sns.pointplot(x=list(sse.keys()), y=list(sse.values()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F7** \n",
    "\n",
    ">a. Vilket k är lämpligast enligt armbågsmetoden? Varför?\n",
    ">\n",
    ">b. Fyll i detta tal i nedanstående kod som skapar modellen genom att ersätta punkterna med rätt siffra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KMeans(n_clusters= ..., random_state=42)\n",
    "model.fit(customers_normalized)\n",
    "model.labels_.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vad säger vår modell?\n",
    "\n",
    "För att undersöka vad vi fått för kluster kan vi räkna ut medelvärdena för våra skapade kluster. Först lägger vi dock till en kolumn för våra kluster i vår dataframe.\n",
    "\n",
    "Vi kan också visualisera våra kluster för att få en tydligare bild av dem.\n",
    "\n",
    ">\n",
    ">a. Vad säger dessa värden om våra kunder?\n",
    ">\n",
    ">b.Vad betyder värdena i den sista kolumnen, som också är ny?\n",
    ">\n",
    ">c. Vad säger visualiseringen?\n",
    ">\n",
    ">d.Baserat på vad vi nu vet, döp de kluster som skapats till lämpliga namn och skriv en kort rekommendation till butiken hur de bör hantera dem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers[\"Cluster\"] = model.labels_\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.groupby('Cluster').agg({\n",
    "    'Recency':'mean',\n",
    "    'Frequency':'mean',\n",
    "    'MonetaryValue':['mean', 'count']}).round(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_normalized = pd.DataFrame(customers_normalized, columns=['Recency', 'Frequency', 'MonetaryValue'])\n",
    "df_normalized['ID'] = customers.index\n",
    "df_normalized['Cluster'] = model.labels_\n",
    "df_normalized.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nor_melt = pd.melt(df_normalized.reset_index(),\n",
    "                      id_vars=['ID', 'Cluster'],\n",
    "                      value_vars=['Recency','Frequency','MonetaryValue'],\n",
    "                      var_name='Attribute',\n",
    "                      value_name='Value')\n",
    "df_nor_melt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lineplot('Attribute', 'Value', hue='Cluster', data=df_nor_melt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Skriv din rekommendation för de namngivna klustren i denna markdown,ersätt denna text.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Del 2 Associationsanalys \n",
    "\n",
    "Inspirerad av: David Johnsson\n",
    "\n",
    "Associationsanalys är en oövervakad analysmetod som går ut på att hitta relationer i data. Ett open source bibliotek som har funktioner för att hantera associationsanalys är exempelvis `mlxtend` som vi kommer använda ilaborationen. Läs mer om `mlxtend` [här](https://rasbt.github.io/mlxtend/). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Importera bibliotek och läs in data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"groceries.csv\") as groceries_file:\n",
    "    dataset = list(csv.reader(groceries_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "F1. Ta reda på hur filen ser ut genom att öppna den i en vanlig text editor. Fundera sedan på varför vi inte kommer läsa in den i en dataframe som vi ofta gjort hitills.\n",
    "\n",
    ">\n",
    ">a.Vad för typ av data innehåller filen? \n",
    ">\n",
    ">b.Varför är det olämpligt att använda en dataframe för att läsa in filen?\n",
    ">\n",
    "\n",
    "Istället för en dataframe kommer vi läsa in filen i en så kallad sparse matrix som vi sedan kan titta på med en dataframe. Nedan är ett exempel på en sparse matrix för 3 olika kunder som köpt 3 produkter var. Varje produkt som finns får sin egen kolumn medan varje rad representerar en shoppingvagn som någon kund köpt. \n",
    "\n",
    "\n",
    "| &nbsp; | Produkt 1 | Produkt 2 | Produkt 3 |\n",
    "|:------:|:---------:|:---------:|:---------:|\n",
    "| Kund 1 |     0     |     0     |     1     |\n",
    "| Kund 2 |     1     |     1     |     0     |\n",
    "| Kund 3 |     0     |     1     |     0     |\n",
    "\n",
    "\n",
    ">\n",
    ">c. Vad är fördelen med att använda en sparse matrix jämfört med att läsa in .csv-filen som den är?\n",
    ">\n",
    ">d. Vad finns det för nackdel?\n",
    ">\n",
    ">e. Hur ser matrisen ut i jämförelse med .csv-filen?\n",
    "\n",
    "\n",
    "I `mlxtend` finns en `TransactionEncoder()` klass för att skapa en sparse matrix av vår .csv-fil. Nedan kod uför detta.\n",
    "\n",
    ">\n",
    ">f. Kommentera koden nedan så att man förstår vad som händer.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te = TransactionEncoder()\n",
    "te_ary = te.fit(dataset).transform(dataset)\n",
    "groceries = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "groceries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Summera och inspektera transaktionerna. \n",
    "\n",
    "Som vanligt behöver vi göra oss familiära med den data vi har, innan vi kan påbörja arbetet med att generera associationsregler.\n",
    "\n",
    "**F2** Använd lämpliga funktioner som ni lärt er i tidigare laborationer för att skapa en bild av hur er sparse matrix ser ut, besvara också följande frågor. \n",
    ">\n",
    ">a. Beräkna densiteten på matrisen. TIPS! För att beräkna densiteten behöver ni veta hur många värden som innehåller `True` respektive `False`.\n",
    ">\n",
    ">b. Hur många produkter finns i ert dataset?\n",
    ">\n",
    ">c.Hur många transaktioner finns i ert dataset?\n",
    ">\n",
    ">d.Vilka är de 10 vanligaste produkterna i ert dataset? Svaret ska ges i form av en lista med strängar ex. `[\"potatis\", \"köttbullar\", mjölk,..]`\n",
    ">\n",
    ">e.Hur många transaktioner innehåller produkten \"soda\"?\n",
    ">\n",
    ">f.Hur många transaktioner innehåller endast en produkt?\n",
    ">\n",
    ">g.Hur många produkter är det i den transaktion som innehåller flest produkter?\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Frekvensen av produkter \n",
    "\n",
    "**F3** Följande funktion `item_frequency()` beräknar antalet av en viss produkt i förhållande tilldet totala antalet transaktioner i %. \n",
    "\n",
    ">\n",
    ">a. Använd denna  funktion (alltså anropa funktionen, gör inga ändringar i själva funktionen) för att beräkna frekvensen av följande produkter: \"whole milk\", \"butter\", \"rice\". \n",
    ">\n",
    ">b. Använd funktionen för att beräkna frekvensen av de produkter som finns på rad 4,5 och 6. \n",
    ">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_frequency(dataset):\n",
    "    return dataset.sum() / len(dataset) * 100\n",
    "\n",
    "item_frequency(groceries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mer intressant för vår kommande analys är att veta vilka produkter som förekommer fler gånger än ett visst bestämt värde vilken kallas support. \n",
    "\n",
    "**F4** Definiera en funktion `item_frequency_plot` som tar ett dataset och ett supportvärde som parametrar och returnerar dessa produkter och deras frekvens i %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def item_frequency_plot(dataset, support): \n",
    "    \n",
    "    #er kod här"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**F5** Använd din funktion för att plotta ut de produkter som har en support på minst 0.125 i ett stapeldiagram. (TIPS! använd koden nedan och skicka endast med de korrekta parametrarna för att skapa ett stapeldiagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = item_frequency_plot().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "## Extrahera associationsregler \n",
    "\n",
    "*MatFörAlla* börjar bli otåliga och tycker att er analys hitills endast visat på saker de redan vet. De vill att ni ska ta fram ny kunskap somär till nytta för dem om deras kunder.\n",
    "\n",
    "Det är därmed dax att extrahera associationsregler för att kunna hjälpa *MatFörAlla* att placera sina produkter rätt i butiken. \n",
    "\n",
    "Det finns tre olika typer av associeringsregler:*Trivial*, *Unexplainable* och *Actionable* \n",
    "\n",
    "* Actionable - Målet med en market basket analysis är qtt hitta associationer som går att agera på, alltså som ger användbar information.  \n",
    "* Trivial - Regler som är uppenbara och därmed inte intressanta. \n",
    "* Unexplainable - Sambandet som regeln står för går inte att förklara utan ytterligare forskning. \n",
    "\n",
    "**F6**\n",
    "\n",
    ">\n",
    ">a. Förklara varje regeltyp, vad innebär de för vårt arbete? Ge exempel på de tre regeltyperna utifrån domänen matvaruhandel.\n",
    ">\n",
    ">b. Ge ett exempel på varje regel utifrån den data ni har fått av *MatFörAlla*\n",
    ">\n",
    ">\n",
    "\n",
    "För att förenkla hur vi kommunicerar kring assosiationsregeler använder vi följande standardiserade sätt att beskriva dem:\n",
    "\n",
    "$Antecedent \\rightarrow Consequent$.\n",
    "\n",
    "Exempel:\n",
    "\n",
    "$Toys, wrapping paper \\rightarrow Batteries$ \n",
    "\n",
    "Ovanstående tolkas som att om du köper leksaker och inslagningspapper ($Antecedent$) är det troligt att du också köper batterier ($Consequent$)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Att mäta associeringsregler \n",
    "\n",
    "För att ta reda på vilka associationsregler som är värdefulla krävs mycket domänkunskap. Det finns dock också några mätvärden som kan användas för att hjälpa till att avgöra kvaliteten på reglerna och för att veta hur mycket vikt vi bör lägga vid en specifik regel. \n",
    "\n",
    "Det finns tre huvudsakliga sätt att mäta associeringsregler:\n",
    "\n",
    "**Support**\n",
    "\n",
    "Support är antalet transaktioner som innehåller ett specifierat antal produkter. Ju oftare dessa produkter förekommer gemensamt (alltså idetta fallet köpts gemensamt) desto större blir vikten av supporten.\n",
    "\n",
    "Om transaktionsdata ser ut enligt följande:\n",
    "\n",
    "```\n",
    "t1: Beef, Carrot, Milk\n",
    "t2: Steak, Cheese\n",
    "t3: Cheese, Flingor\n",
    "t4: Steak, Carrot, Cheese\n",
    "t5: Steak, Carrot, Butter, Cheese, Milk\n",
    "t6: Carrot, Butter, Milk\n",
    "t7: Carrot, Milk, Butter\n",
    "```\n",
    "\n",
    "Skulle supporten för att kombinationen morötter, smör och mjölk köps tillsammans se ut enligt följande:\n",
    "\n",
    "$$Support(Carrot \\land Butter \\land Milk) = \\frac{3}{7} = 0.43$$\n",
    "\n",
    "detta på grund av att en kombination av dessa tre produkter förekommer 3 gånger av 7 möjliga transaktioner. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Confidence**\n",
    "\n",
    "Konfidens innebär att om vi har en regel som säger följande:  $Beef, Chicken \\rightarrow Apple$ med en konfidens på 33%, så innebär det att om det finns biff och kyckling i någons shoppingvagn så är det 33% chans att det också finns äpplen. \n",
    "\n",
    "Konfidensen beräknas exempelvis såhär: \n",
    "\n",
    "Givet följande regel: $Butter \\rightarrow Milk, Chicken$\n",
    "\n",
    "$$Butter \\rightarrow Milk, Chicken = \\frac{Support (Butter \\land Milk \\land Chicken)}{Support (Butter)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**Lift**\n",
    "\n",
    "Lift ger oss ett mätvärde på hur bra en regel är, baserat enbart på den högra sidan av en regel(alltså $Consequent$). Detta innebär att exempelvis regler som inkluderar vanliga produkter som $Consequent$ så kommer reglen inte säga någoting av värde. Det är alltså inte meningsfullt att ha mjölk, som är en väldigt vanlig produkt, på den högra sidan i en regel. \n",
    "\n",
    "Tumregeln för Lift är följande: \n",
    "\n",
    "Om Lift är  $>1$  så är regeln bättre än att gissa.Om Lift är $\\leq1$ så är regeln ungefär likvärdig med en ren gissning. \n",
    "\n",
    "\n",
    "Exempel:\n",
    "\n",
    "$$Chicken \\rightarrow Milk = \\frac{Support (Chicken \\land Milk)}{Support(Chicken) \\times Support (Milk)} = \\frac{(4 / 7)}{(5 / 7) \\times (4 / 7)} = 1.4$$\n",
    "\n",
    "Detta implicerar att  $Chicken \\rightarrow Milk$ skulle kunna vara en bra regel eftersom $1.4 > 1$. Om vi ändrar support för hur ofta mjölk inhandlas till $6 / 7$ istället så blir resultatet ett annat.\n",
    "\n",
    "$$Chicken \\rightarrow Milk = \\frac{Support (Chicken \\land Milk)}{Support(Chicken) \\times Support (Milk)} = \\frac{(4 / 7)}{(5 / 7) \\times (6 / 7)} = 0.933$$\n",
    "\n",
    "Nu ser samma regel: $Chicken \\rightarrow Milk$ inte längre ut som en bra regel eftersom $0.933 < 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F7** Förklara de två matematiska symbolerna $\\land$ och $\\times$, vad betyder de?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "### Utföra associationsanalys i Python \n",
    "\n",
    "Nu är det dags att hitta associationsregler för vårt dataset. Vi startar med att definiera defaultvärden för support och confidense.\n",
    "\n",
    "En viktig funktion för att utföra associationsanalys är funktionen `apriori()` som beräknar support (alltsåfrekvens av produkter)på ett liknande sätt som gjordes i början av denna laboration. Funktionen har dock några ytterligare egenskaper som gör den lämplig att använda för våra syften. Exempelvis kan vi ange minsta support i funktionen.\n",
    "\n",
    "**F8**\n",
    ">\n",
    ">a.Testa att använda funktionen `apriori()` med vårt givna dataset, samt ange att minimisupport ska vara 0,5%, spara och skriv ut resultatet i en variabel som heter `frequent_itemsets`. \n",
    ">\n",
    ">b.Förutom att beräkna support,vad gör funktionen `apriori()`?\n",
    ">\n",
    ">c.Vad för struktur består resultatet av?\n",
    ">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequent_itemsets = #kod här"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "Som ni förmodligen sett vid det här laget så skapar funktionen `apriori()` inga regler. För att generera dessa används funktionen `association_rules()` i nedan kod skapas assosiationsregler för hela ert dataset. Det är också möjligt att undersöka de olika mätvärdena (`support`, `confidence` och `lift`) genom att specifiera att dessa ska visas samt vad lägsta tröskelvärdet ska vara. I koden nedan är detta gjort för mätvärdet `confidence`\n",
    "\n",
    "Om du istället vill skapa regler endast för utvalda delar av ett dataset så är det också möjligt.\n",
    "\n",
    "**F9**\n",
    "\n",
    ">a.Utifrån resultatet av funktionen `apriori()`, vad kan vara en lämplig filtrering för att utesluta ointressanta delar av ert dataset innan ni skapar regler? Varför?\n",
    ">\n",
    ">b.Filtrera bort irrelevant data och skapa sedan nya associationsregler för ert filtrerade dataset.\n",
    ">\n",
    ">c.Undersök samtliga tre mätvärden för de nya associationsreglerna för ditt filtrerade dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_rules = association_rules(frequent_itemsets, metric=\"confidence\", min_threshold=0.1)\n",
    "grocery_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Det är också möjligt att lägga till kolumner för att räkna ut storleken dvs antalet produkter som är  $Antecendants$ i varje regel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grocery_rules[\"num_antecedents\"] = grocery_rules[\"antecedents\"].apply(lambda x: len(x))\n",
    "grocery_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**F10** \n",
    "\n",
    ">\n",
    ">a. hur många regler har skapats totalt?\n",
    ">\n",
    ">b.Hur många regler har 3 eller fler $Antecendants$?\n",
    ">\n",
    ">c. Vad gör en regel intressant?\n",
    ">\n",
    ">d.Finns det någon/några av de regler ni skapat med fler än 3 $Antecendants$ som är värda att undersöka närmare?\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**F11** \n",
    "\n",
    "Det är ofta värdefullt att veta vilka regler som inkluderar specifika produkter. \n",
    "\n",
    ">\n",
    ">a. Varför kan detta vara intressant i vårt sammanhang (alltså *MatFörAlla*)\n",
    ">\n",
    ">b.Filtrera ut alla regler som innehåller produkten citrusfrukt som $Antecendant$ .\n",
    ">\n",
    ">c.Utforska dessa regler med avseende på konfidence, support och lift.\n",
    ">\n",
    ">d.Baserat på era regler, vad för rekommendationer kan ni ge *MatFörAlla* gällande hur de bör placera citrusfrukter i butiken? Förklara svaret.\n",
    ">\n",
    ">e.Ta själv fram en artikel som du identifierat som intressant och använd den för att skapa en rekommendation till *MatFörAlla*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lang": "en"
   },
   "source": [
    "**F12**\n",
    "\n",
    ">a.Givet att *MatFörAlla* också skulle kunna samla in data om vilka kunder som köpt vad istället för bara vad som köpts baserat på ID, vilka\n",
    ">ytterligare möjlgheter för denna typ av analys kan sådan data ge?\n",
    ">\n",
    ">b.En kund köper ett ljus och 20burkar med öl, vilket påverkan har det på er analys och kvaliteten på den? \n",
    ">\n",
    ">c.Hur påverkas er analys av tiden? Vad ställer det för krav på *MatFörAlla*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "När ni är färdiga lämnar ni in i Studium, antingen som html-fil eller med länk till Collaboratory. \n",
    "\n",
    "**Lycka till!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "nbTranslate": {
   "displayLangs": [],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "sv",
   "targetLang": "en",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
